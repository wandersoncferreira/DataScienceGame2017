{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from greenpyce.feature_engineering import TargetEncoder\n",
    "from greenpyce.feature_engineering import onehot\n",
    "from greenpyce.feature_engineering import RankCategorical\n",
    "from greenpyce.feature_engineering import LabelCount\n",
    "from greenpyce.feature_engineering import time_count_between_dates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# The universe and everything else\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_report(y_test, y_pred):\n",
    "    print(\"*********************************\")\n",
    "    print(\"Accuracy: {0:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"Precision: {0}\".format(metrics.precision_score(y_test, y_pred)))\n",
    "    print(\"Recall: {0}\".format(metrics.recall_score(y_test, y_pred)))\n",
    "    print(\"F1-Score: {0}\".format(metrics.f1_score(y_test, y_pred)))\n",
    "    conf_m = confusion_matrix(y_test, y_pred)\n",
    "    print(conf_m)\n",
    "    try: \n",
    "        print(\"Roc Area: {0}\".format(metrics.roc_auc_score(y_test, y_pred))) \n",
    "        roc_curve_label(y_test, y_pred)\n",
    "    except:\n",
    "        print(\"--\")\n",
    "    print(\"*********************************\")\n",
    "    \n",
    "    return metrics.roc_auc_score(y_test, y_pred)\n",
    "        \n",
    "def roc_curve_label(y_test, y_pred):\n",
    "    plt.figsize=(10,6)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='RT + LR')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def expand_date_info(df, columns, hour_feats=False):\n",
    "    \"\"\"\n",
    "    Create year, month, day, hour and minute from a datetime column.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df[column + \"_DAY\"] = df[column].apply(lambda x: x.day)\n",
    "        df[column + \"_MONTH\"] = df[column].apply(lambda x: x.month)\n",
    "        df[column + \"_YEAR\"] = df[column].apply(lambda x: x.year)\n",
    "        if hour_feats:\n",
    "            df[column + \"_HOUR\"] = df[column].apply(lambda x: x.hour)\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_date(x):\n",
    "    text = \"\"\n",
    "    year = int(x / 10000)\n",
    "    month = int((x % 10000) / 100)\n",
    "    day = (x % 100)\n",
    "    text = str(year) + \"-\" + str(month) + \"-\" + str(day)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def build_kaggle_submission(model, df_test, features, filename=\"my_submission\"):\n",
    "    \"\"\"\n",
    "    Build a compressed Kaggle Submission\n",
    "    \n",
    "    df_test : Dataframe com os dados de teste\n",
    "    model : Modelo treinado\n",
    "    features : Lista de features usadas para o treinamento\n",
    "    filename : Nome do arquivo de submissÃ£o\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = model.predict_proba(df_test[features])\n",
    "    preds = preds[:, 1]\n",
    "    \n",
    "    with gzip.open(filename + '.csv.gz', 'wt') as outf:\n",
    "        fo = csv.writer(outf, lineterminator='\\n')\n",
    "        fo.writerow([\"sample_id\", \"is_listened\"])       \n",
    "        for i, pred in enumerate(preds):\n",
    "            fo.writerow([i, pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_model(model, X, y, params_grid, cross_validation):\n",
    "    model_tmp = copy(model)\n",
    "    search = GridSearchCV(estimator=model_tmp, param_grid=params_grid, cv=cross_validation)\n",
    "    search.fit(X, y)\n",
    "    \n",
    "    print(\"Best score: \", search.best_score_)\n",
    "    print(\"Best params: \", search.best_params_)\n",
    "    print(\"Function used to choose the best parameter for the model: \", search.scorer_)\n",
    "    \n",
    "    # input the best params inside the model and return the model with the right params\n",
    "    best_params = search.best_params_\n",
    "    model_tmp = model_tmp.set_params(**best_params)\n",
    "    return model_tmp, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "nmidia_feats = pd.read_csv(\"../features/nmidia_feats.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmidia_feats[\"user_id\"] = nmidia_feats[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_used = [\"user_id\", \"nartist_with_flow\", \"nartist_with_flow_listened\", \"nmidia_regular_listened\", \"nmidia_regular\",\n",
    "                      \"nmidia_with_flow\", \"nartist_regular_listened\", \"nartist_regular\", \"nmidia_with_flow_listened\"]\n",
    "df_user_feats = nmidia_feats[user_features_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_user_feats, on=\"user_id\")\n",
    "df_test = pd.merge(df_test, df_user_feats, on=\"user_id\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"year\"] = df_train[\"release_date\"].apply(lambda x: x // 10000)\n",
    "df_train.drop(df_train[df_train[\"year\"] > 2017].index , inplace=True)\n",
    "df_train[\"text\"] = df_train[\"release_date\"].apply(lambda x : parse_date(x))\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda x : pd.to_datetime(x))\n",
    "df_train[\"release_date\"] = df_train[\"text\"]\n",
    "expand_date_info(df_train, [\"release_date\"]).head()\n",
    "\n",
    "\n",
    "df_test[\"year\"] = df_test[\"release_date\"].apply(lambda x : x // 10000)\n",
    "df_test.drop(df_test[df_test[\"year\"] > 2017].index , inplace=True)\n",
    "df_test[\"text\"] = df_test[\"release_date\"].apply(lambda x : parse_date(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda x : pd.to_datetime(x))\n",
    "df_test[\"release_date\"] = df_test[\"text\"]\n",
    "expand_date_info(df_test, [\"release_date\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop([\"text\", \"release_date\"], axis=1, inplace=True)\n",
    "df_test.drop([\"text\", \"release_date\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"media_duration\"] = df_train[\"media_duration\"].apply(lambda x: round(x / 60.0))\n",
    "df_test[\"media_duration\"] = df_test[\"media_duration\"].apply(lambda x : round(x / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_for_target_encoder = [\"genre_id\", \"context_type\", \"user_id\", \"media_id\", \"media_duration\"]\n",
    "cols_for_onehot = [\"platform_name\", \"platform_family\", \"listen_type\", \"user_gender\", \"user_age\"]\n",
    "cols_for_rank = [\"artist_id\"]\n",
    "\n",
    "target = \"is_listened\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te = TargetEncoder(cols_for_target_encoder, \"is_listened\", inplace=True)\n",
    "te.fit(df_train)\n",
    "\n",
    "te.transform(df_train)\n",
    "te.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = RankCategorical(cols_for_rank)\n",
    "rc.fit(df_train)\n",
    "rc.transform(df_train)\n",
    "rc.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = onehot(df_train, cols_for_onehot)\n",
    "df_test = onehot(df_test, cols_for_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df_train.columns.values if col != target]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier params to be used in GridSearch\n",
    "\n",
    "parametros utilizados no modelo:\n",
    "\n",
    "    - n_estimators = numero de arvores\n",
    "    - criterion = funcao que mede a qualidade dos splits. OpÃ§Ãµes sÃ£o gini que mede a impureza Gini e \"entropy\" que mede o ganho de informaÃ§Ã£o.\n",
    "    - max_features = o numero de features que deve ser considerada para cada split\n",
    "    - max_depth = profundidade das arvores\n",
    "    - oob_score = utiliza leave-one-out para calcular acurÃ¡cia de validaÃ§Ã£o fora da amostra   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {\"n_estimators\": [3, 6, 9, 12, 15], \n",
    "               \"criterion\": [\"gini\", \"entropy\"],\n",
    "               \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n",
    "               \"max_depth\": [5, 10, 15, 20, 30, None],\n",
    "               \"oob_score\": [False, True],\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df_train[features]\n",
    "y = df_train[target]\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "best_model, best_params = optimize_model(model, X, y, grid_params, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AplicaÃ§Ã£o do modelo RandomForest com os melhores parametros encontrados pelo GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train[features], df_train[target], test_size = 0.2)\n",
    "best_model.fit(X_train, y_train)\n",
    "best_model.score(X_test, y_test)\n",
    "complete_report(y_test, best_model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
